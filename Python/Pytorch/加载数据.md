
**torch.utils.data**

## IterableDataset

`torch.utils.data.IterableDataset` 是 PyTorch 中用于处理**流式数据**或**超大规模数据**的特殊数据集类。

它与你通常使用的标准 `Dataset`（称为 Map-style Dataset，即实现了 `__getitem__` 和 `__len__` 的类）有本质的区别。

以下是核心概念、适用场景以及一个必须注意的“大坑”。

---

### 1. 核心区别：Map-style vs. Iterable-style

#### A. Map-style Dataset (标准 Dataset)

- **核心方法：** `__getitem__(self, index)`
    
- **工作方式：** 像查字典一样。DataLoader 告诉它：“给我第 5 个样本”，“给我第 100 个样本”。
    
- **前提：** 你必须知道数据的总长度，且支持**随机访问**（Random Access）。
    
- **比喻：** 书架上的书，想拿哪本拿哪本。
    

#### B. IterableDataset (流式 Dataset)

- **核心方法：** `__iter__(self)`
    
- **工作方式：** 像流水线一样。DataLoader 只能说：“给我下一个数据”。它无法跳跃，只能顺序读取。
    
- **前提：** 数据量可能无限大，或者不知道总长度，或者随机读取成本极高（例如从网络流读取）。
    
- **比喻：** 观看直播电视，只能顺着时间看，不能随意跳到未来的某一帧。
    

---

### 2. 什么时候应该用 IterableDataset？

根据你之前的技术栈（MinIO, RustFS, 大模型训练），以下场景**强烈建议**使用 IterableDataset：

1. **数据远大于内存：** 你的训练集有 TB 级别，无法一次性生成索引列表。
    
2. **从对象存储流式读取：** 你不想把 MinIO/S3 上的文件全部下载到本地硬盘，而是想**边下载边训练**（Streaming）。
    
3. **动态生成数据：** 数据是实时生成的（例如强化学习中的环境交互），没有固定的“索引”。
    

---

### 3. 代码实现示例

这是一个最基础的实现，数据不是通过索引获取，而是通过 Python 的生成器（`yield`）产出。

Python

```python
import torch
from torch.utils.data import IterableDataset, DataLoader

class MyStreamDataset(IterableDataset):
    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __iter__(self):
        # 这里是一个生成器，每次 yield 一个样本
        for i in range(self.start, self.end):
            yield i

# 使用
dataset = MyStreamDataset(0, 10)
loader = DataLoader(dataset, batch_size=2)

for batch in loader:
    print(batch)
# 输出: tensor([0, 1]), tensor([2, 3]) ...
```

---

### 4. ⚠️ 最大的坑：多进程重复读取 (`num_workers > 0`)

这是使用 `IterableDataset` 最容易翻车的地方。

当你设置 `DataLoader(..., num_workers=2)` 时，PyTorch 会启动 2 个进程，**每个进程都会完整地复制一份 Dataset 对象**。

- **结果：** 如果你的 Dataset 是产生 0~9 的数字，两个进程都会产生 0~9。你的模型会**把同样的数据学两遍**（甚至更多）。
    

#### 解决方案：在 `__iter__` 中通过 `get_worker_info` 切分数据

你需要告诉每个 Worker：“你只负责读你那一部分”。

Python

```python
import math
from torch.utils.data import get_worker_info

class SafeStreamDataset(IterableDataset):
    def __init__(self, data_source):
        self.data_source = data_source

    def __iter__(self):
        worker_info = get_worker_info()
        
        if worker_info is None:
            # 单进程模式：读取所有数据
            iter_start = 0
            iter_end = len(self.data_source)
        else:
            # 多进程模式：计算当前 Worker 应该读哪一部分
            # worker_id: 当前进程编号 (0, 1, ...)
            # num_workers: 总进程数
            per_worker = int(math.ceil(len(self.data_source) / float(worker_info.num_workers)))
            worker_id = worker_info.id
            
            iter_start = worker_id * per_worker
            iter_end = min(iter_start + per_worker, len(self.data_source))
            
        # 只 yield 当前 worker 分配到的数据
        for i in range(iter_start, iter_end):
            yield self.data_source[i]
```

### 总结建议

|**特性**|**标准 Dataset (Map-style)**|**IterableDataset**|
|---|---|---|
|**随机打乱 (Shuffle)**|**容易** (DataLoader 直接通过索引随机取)|**困难** (只能在内存缓冲区里局部打乱，无法全局打乱)|
|**实现难度**|简单|中等 (需处理多进程切分)|
|**适用场景**|图像分类、本地文件、中小规模 NLP|**超大数据集、S3/MinIO 流式读取、语音流**|




